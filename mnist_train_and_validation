# -*- coding: utf-8 -*-
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.contrib.framework import arg_scope
from tensorflow.contrib.layers import batch_norm, flatten


def conv_layer(inputs, filter, kernel, stride, padding='SAME', layer_name="conv"):
    with tf.name_scope(layer_name):
        network = tf.layers.conv2d(inputs=inputs, filters=filter, kernel_size=kernel,
                                   strides=stride, padding=padding, use_bias=False)
        return network


def Batch_Normalization(x, training, scope):
    with arg_scope([batch_norm],
                   scope=scope,
                   updates_collections=None,
                   decay=0.9,
                   center=True,
                   scale=True,
                   zero_debias_moving_mean=True):
        return tf.cond(training,
                       lambda: batch_norm(inputs=x, is_training=training, reuse=None),
                       lambda: batch_norm(inputs=x, is_training=training, reuse=True))


def Fully_connected(x, units=10, layer_name='fully_connected') :
    with tf.name_scope(layer_name) :
        return tf.layers.dense(inputs=x, use_bias=False, units=units)


def Net(inputs, training):
    inputs = tf.reshape(inputs, (-1, 28, 28, 1))
    conv1 = conv_layer(inputs, filter=32, kernel=[3, 3], stride=1, layer_name="conv1")
    relu1 = tf.nn.relu(conv1)
    relu1_bn = Batch_Normalization(relu1, training, "relu1_bn")
    pool1 = tf.layers.max_pooling2d(relu1_bn, pool_size=[2, 2], strides=2, padding="SAME")

    conv2 = conv_layer(pool1, filter=64, kernel=[3, 3], stride=1, layer_name="conv2")
    relu2 = tf.nn.relu(conv2)
    relu2_bn = Batch_Normalization(relu2, training, "relu2_bn")
    pool12 = tf.layers.max_pooling2d(relu2_bn, pool_size=[2, 2], strides=2, padding="SAME")

    conv2 = conv_layer(pool12, filter=128, kernel=[3, 3], stride=1, layer_name="conv3")
    fla_con = flatten(conv2)
    fc = Fully_connected(fla_con)

    return fc


def train_mnist(mnist):

    x = tf.placeholder(dtype=tf.float32, shape=[None, 784], name='x')
    y = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='label')

    training_flag = tf.placeholder(tf.bool)

    logits = Net(x, training_flag)

    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)
    loss = tf.reduce_mean(cross_entropy, axis=0, name='loss')

    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
    train = optimizer.minimize(loss)

    correct_nums = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_nums, tf.float32))

    saver = tf.train.Saver(tf.global_variables())

    with tf.Session() as sess:

        tf.global_variables_initializer().run()
        tf.local_variables_initializer().run()

        summary_writer = tf.summary.FileWriter("log", sess.graph)

        total_epochs = 20
        batch_size = 100

        for epochs in xrange(1, total_epochs+1):

            total_nums = 55000
            iteration = total_nums / batch_size



            # train the network
            train_loss = 0.0
            train_acc = 0.0
            for i in xrange(iteration):
                batch_x, batch_y = mnist.train.next_batch(batch_size)
                _, batch_loss, batch_acc = sess.run([train, loss, accuracy],
                                                 feed_dict={x: batch_x, y: batch_y,
                                                            training_flag: True})
                train_loss += batch_loss
                train_acc += batch_acc

            ave_loss = train_loss*1.0/iteration
            ave_acc = train_acc*1.0/iteration
            print 'train:', ave_loss, ave_acc

            train_summary = tf.Summary(
                value=[tf.Summary.Value(tag='train_loss', simple_value=ave_loss),
                       tf.Summary.Value(tag='train_acc', simple_value=ave_acc)])

            # test
            test_acc = 0.0
            test_loss = 0.0

            test_batch_size = 100
            test_iteration = 5000 / test_batch_size

            for i in xrange(test_iteration):
                test_batch_x = mnist.validation.images
                test_batch_y = mnist.validation.labels

                test_batch_loss, test_batch_acc = sess.run([loss, accuracy],
                                                           feed_dict={x: test_batch_x,
                                                                      y:test_batch_y,
                                                                      training_flag:False})
                test_loss += test_batch_loss
                test_acc += test_batch_acc

            test_ave_loss = test_loss*1.0/test_iteration
            test_ave_acc = test_acc*1.0/test_iteration
            print 'test:', test_ave_loss, test_ave_acc

            test_summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=test_ave_loss),
                                             tf.Summary.Value(tag='test_acc', simple_value=test_ave_acc)])


            # write to logdir
            summary_writer.add_summary(summary=train_summary, global_step=epochs)
            summary_writer.add_summary(summary=test_summary, global_step=epochs)
            #summary_writer.add_summary()
            summary_writer.flush() # make sure the things you want was written in ssd

            # save model
            saver.save(sess=sess, save_path="model/mnist.ckpt")


if __name__  == "__main__":

    mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
    train = mnist.train.images
    train_label = mnist.train.labels
    validation = mnist.validation.images
    validation_label = mnist.validation.labels
    test = mnist.test.images
    test_label = mnist.test.labels

    print train.shape
    print train_label.shape
    print validation.shape
    print validation_label.shape
    print test.shape
    print test_label.shape

    #train(train, train_label)
    train_mnist(mnist)




